{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized word vector with pretrain japanese word2vec \n",
    "\n",
    "https://github.com/Kyubyong/wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = \"-word2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cPickle (python2.7)\n",
    "#http://testpy.hatenablog.com/entry/2017/03/17/000626\n",
    "import _pickle as cPickle\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 10\n",
    "num_classes = 3\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pickle file contains [revs, W, W2, word_idx_map, vocab] # W2 random vectors\n",
    "x = cPickle.load(open(\"tweet.p\",\"rb\"), encoding=\"latin1\") # Add encoding=\"latin1\" because got UnicodeDecodeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset content\n",
    "Get data from twitter (reference: http://tech.wonderpla.net/entry/2017/10/10/110000)\n",
    "- Label 0\n",
    "    - KEYWORD = \"芸能 OR アニメ OR 漫画 OR ドラマ OR ゲーム\"            #エンタメ系のキーワード\n",
    "    - CLASS_LABEL = \"\\__label__0\"\n",
    "\n",
    "- Label 1\n",
    "    - KEYWORD = \"美容 OR サロン OR エステ OR 化粧 OR 保湿\"            #美容系のキーワード\n",
    "    - CLASS_LABEL = \"\\__label__1\"\n",
    "\n",
    "- Label 2\n",
    "    - KEYWORD = \"日常 OR 料理 OR 家事 OR 収納 OR 家具\"            #暮らし系のキーワード\n",
    "    - CLASS_LABEL = \"\\__label__2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>アサデス 。 6 時 台 相 葉 雅紀 10 月 期 スタート 金曜 ナイト ドラマ 「 僕...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>森田 童子 さん 死去 　 ドラマ 「 高校 教師 」 の 主題歌 で 話題 （ 朝日新聞 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_words  split                                               text  y\n",
       "543         26      2  アサデス 。 6 時 台 相 葉 雅紀 10 月 期 スタート 金曜 ナイト ドラマ 「 僕...  0\n",
       "383         55      6  森田 童子 さん 死去 　 ドラマ 「 高校 教師 」 の 主題歌 で 話題 （ 朝日新聞 ...  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(x[0])\n",
    "\n",
    "# label 0: Entertainment \n",
    "df[df['y'] == 0].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>37</td>\n",
       "      <td>9</td>\n",
       "      <td>＼ フォロー プレゼント ／ 初夏 の 美容 特集 番外 編 ️ 疲れ やすい 、 眠れ な...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>インターネット 囲碁 サロン オンライン 対極 # 囲碁 # 対極 # オンライン # イン...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_words  split                                               text  y\n",
       "989         37      9  ＼ フォロー プレゼント ／ 初夏 の 美容 特集 番外 編 ️ 疲れ やすい 、 眠れ な...  1\n",
       "679         13      2  インターネット 囲碁 サロン オンライン 対極 # 囲碁 # 対極 # オンライン # イン...  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label 1 : Beauty\n",
    "df[df['y'] == 1].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>58</td>\n",
       "      <td>9</td>\n",
       "      <td>万引き 家族 、 貧しい 家 は 物 が 多く ゴチャゴチャ し て い て 、 裕福 な ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>日常 生活 に は 支障 ない ん だ けど な 〜 〜 〜 作業 が 辛い</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_words  split                                               text  y\n",
       "1769         58      9  万引き 家族 、 貧しい 家 は 物 が 多く ゴチャゴチャ し て い て 、 裕福 な ...  2\n",
       "1546         16      0             日常 生活 に は 支障 ない ん だ けど な 〜 〜 〜 作業 が 辛い  2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label 2: Life\n",
    "df[df['y'] == 2].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs, W, W2, word_idx_map, vocab = x[0], x[1], x[2], x[3], x[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  100\n"
     ]
    }
   ],
   "source": [
    "# Get the number of the longest sentence\n",
    "max_l = np.max(pd.DataFrame(revs)[\"num_words\"])\n",
    "print(\"max sentence length: \", max_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revs 1772\n",
      "W 9468\n",
      "W2 9468\n",
      "word_idx_map 9467\n",
      "vocab 9467\n"
     ]
    }
   ],
   "source": [
    "print('revs',len(x[0])) # number of sentence\n",
    "print('W', len(x[1])) # W are pretrained word vectors (unknown words are randomly initialized)\n",
    "print('W2', len(x[2])) # W2 are randomly initialized vectors\n",
    "print('word_idx_map', len(x[3]))\n",
    "print('vocab', len(x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 0,\n",
       " 'text': 'え 、 サラリーマン し ながら 、 商業 漫画 の 仕事 を し て 、 ツイッター に 定期 的 に 漫画 を あげ て 、 さらに コミケ の 原稿 を 作る ！ ？',\n",
       " 'num_words': 32,\n",
       " 'split': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: word2vec vectors\n"
     ]
    }
   ],
   "source": [
    "if word_vectors==\"-rand\":\n",
    "    print(\"using: random vectors\")\n",
    "    U = W2\n",
    "elif word_vectors==\"-word2vec\":\n",
    "    print(\"using: word2vec vectors\")\n",
    "    U = W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9468, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset (check original code)\n",
    "make each sentence an word index map using word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    pad = filter_h - 1\n",
    "    for i in range(pad):\n",
    "        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l + 2*pad:\n",
    "        x.append(0)\n",
    "    return x\n",
    "\n",
    "def make_idx_data_cv(revs, word_idx_map, cv, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, test = [], []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k, filter_h) # one sentence\n",
    "        sent.append(rev[\"y\"])\n",
    "        if rev[\"split\"]== cv:  # \"split\" is random number of np.random.randint(0,10)\n",
    "            test.append(sent)        \n",
    "        else:  \n",
    "            train.append(sent)   \n",
    "    train = np.array(train, dtype=\"int\")\n",
    "    test = np.array(test, dtype=\"int\")\n",
    "    return [train, test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "datasets = make_idx_data_cv(revs, word_idx_map, i, max_l, k=300, filter_h=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of sentence to word index map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 0,\n",
       " 'text': 'え 、 サラリーマン し ながら 、 商業 漫画 の 仕事 を し て 、 ツイッター に 定期 的 に 漫画 を あげ て 、 さらに コミケ の 原稿 を 作る ！ ？',\n",
       " 'num_words': 32,\n",
       " 'split': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,   23, 3829,   22,   15,   28, 3829,   30,\n",
       "       3833,   21, 3845,   29,   15,    2, 3829,   31,    7,   24,   25,\n",
       "          7, 3833,   29, 3842,    2, 3829,   32,   26,   21,   27,   29,\n",
       "       3841, 3843, 3844,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(datasets[0][0]))\n",
    "datasets[0][1] # sentence => word index map padding with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: (1620, 109)\n",
      "test data size: (152, 109)\n"
     ]
    }
   ],
   "source": [
    "print('train data size:', datasets[0].shape)\n",
    "print('test data size:', datasets[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset (using vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_from_sent_2vec(sent, U, word_idx_map, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    pad = filter_h - 1\n",
    "    x = np.zeros((max_l+2*pad, k))\n",
    "\n",
    "    words = sent.split()\n",
    "    # starting after padding\n",
    "    i = pad\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x[i] = U[word_idx_map[word]]\n",
    "            i += 1\n",
    "    return x\n",
    "\n",
    "def make_idx_data_cv_2vec(revs, U, word_idx_map, cv, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train_image, train_label = [], []\n",
    "    test_image, test_label = [], []\n",
    "    test_rev = []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent_2vec(rev[\"text\"], U, word_idx_map, max_l, k, filter_h) # one sentence\n",
    "        if rev[\"split\"] == cv:  # \"split\" is random number of np.random.randint(0,10)\n",
    "            test_image.append(sent) \n",
    "            test_label.append(rev[\"y\"])\n",
    "            test_rev.append(rev)\n",
    "        else:  \n",
    "            train_image.append(sent)\n",
    "            train_label.append(rev[\"y\"])\n",
    "    train_image = np.array(train_image)\n",
    "    train_label = np.array(train_label)\n",
    "    test_image = np.array(test_image)\n",
    "    test_label = np.array(test_label)\n",
    "    test_rev = np.array(test_rev)\n",
    "    return (train_image, train_label), (test_image, test_label, test_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence length(before) 108\n"
     ]
    }
   ],
   "source": [
    "t = \"effective but too tepid biopic\"\n",
    "t_sent_2vec = get_idx_from_sent_2vec(t, U, word_idx_map, max_l, k=300, filter_h=5)\n",
    "print(\"sentence length(before)\", len(t_sent_2vec)) # max_l(51)+2*pad(filter_h-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_sent_2vec[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "datasets_2vec = make_idx_data_cv_2vec(revs, U, word_idx_map, i, max_l, k=300, filter_h=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_image, train_label), (test_image, test_label, test_rev) = datasets_2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of sentence to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 0,\n",
       " 'text': 'え 、 サラリーマン し ながら 、 商業 漫画 の 仕事 を し て 、 ツイッター に 定期 的 に 漫画 を あげ て 、 さらに コミケ の 原稿 を 作る ！ ？',\n",
       " 'num_words': 32,\n",
       " 'split': 2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.014600</td>\n",
       "      <td>0.031418</td>\n",
       "      <td>0.090312</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>-0.013168</td>\n",
       "      <td>-0.069336</td>\n",
       "      <td>0.095244</td>\n",
       "      <td>-0.012322</td>\n",
       "      <td>-0.057443</td>\n",
       "      <td>-0.061566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053096</td>\n",
       "      <td>-0.027628</td>\n",
       "      <td>-0.046655</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.015511</td>\n",
       "      <td>-0.029644</td>\n",
       "      <td>0.006272</td>\n",
       "      <td>-0.025874</td>\n",
       "      <td>-0.076605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.100688</td>\n",
       "      <td>-0.034349</td>\n",
       "      <td>-0.015562</td>\n",
       "      <td>0.102048</td>\n",
       "      <td>-0.034377</td>\n",
       "      <td>-0.029769</td>\n",
       "      <td>-0.074679</td>\n",
       "      <td>0.027827</td>\n",
       "      <td>0.039846</td>\n",
       "      <td>0.077095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050505</td>\n",
       "      <td>-0.074465</td>\n",
       "      <td>0.032439</td>\n",
       "      <td>-0.052896</td>\n",
       "      <td>0.089773</td>\n",
       "      <td>-0.094414</td>\n",
       "      <td>-0.032414</td>\n",
       "      <td>0.082769</td>\n",
       "      <td>-0.039997</td>\n",
       "      <td>0.041615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.015008</td>\n",
       "      <td>0.014014</td>\n",
       "      <td>0.043544</td>\n",
       "      <td>-0.039249</td>\n",
       "      <td>0.007010</td>\n",
       "      <td>0.022439</td>\n",
       "      <td>0.013127</td>\n",
       "      <td>-0.034857</td>\n",
       "      <td>0.049377</td>\n",
       "      <td>-0.051198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071647</td>\n",
       "      <td>0.033772</td>\n",
       "      <td>0.089638</td>\n",
       "      <td>0.014955</td>\n",
       "      <td>0.064228</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>-0.009657</td>\n",
       "      <td>-0.045884</td>\n",
       "      <td>0.019352</td>\n",
       "      <td>0.165582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.119047</td>\n",
       "      <td>0.027210</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.006658</td>\n",
       "      <td>-0.011127</td>\n",
       "      <td>0.037048</td>\n",
       "      <td>0.062418</td>\n",
       "      <td>-0.032071</td>\n",
       "      <td>-0.007870</td>\n",
       "      <td>-0.047811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006505</td>\n",
       "      <td>0.060982</td>\n",
       "      <td>-0.125483</td>\n",
       "      <td>0.103325</td>\n",
       "      <td>0.026611</td>\n",
       "      <td>0.054767</td>\n",
       "      <td>-0.005485</td>\n",
       "      <td>-0.085803</td>\n",
       "      <td>0.017624</td>\n",
       "      <td>-0.100847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.021471</td>\n",
       "      <td>0.029862</td>\n",
       "      <td>-0.038230</td>\n",
       "      <td>-0.037036</td>\n",
       "      <td>-0.000850</td>\n",
       "      <td>0.018548</td>\n",
       "      <td>0.075258</td>\n",
       "      <td>0.027883</td>\n",
       "      <td>0.055498</td>\n",
       "      <td>0.012769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007132</td>\n",
       "      <td>0.028085</td>\n",
       "      <td>-0.053919</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>0.005494</td>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.052201</td>\n",
       "      <td>0.046751</td>\n",
       "      <td>0.104778</td>\n",
       "      <td>0.004859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.100688</td>\n",
       "      <td>-0.034349</td>\n",
       "      <td>-0.015562</td>\n",
       "      <td>0.102048</td>\n",
       "      <td>-0.034377</td>\n",
       "      <td>-0.029769</td>\n",
       "      <td>-0.074679</td>\n",
       "      <td>0.027827</td>\n",
       "      <td>0.039846</td>\n",
       "      <td>0.077095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050505</td>\n",
       "      <td>-0.074465</td>\n",
       "      <td>0.032439</td>\n",
       "      <td>-0.052896</td>\n",
       "      <td>0.089773</td>\n",
       "      <td>-0.094414</td>\n",
       "      <td>-0.032414</td>\n",
       "      <td>0.082769</td>\n",
       "      <td>-0.039997</td>\n",
       "      <td>0.041615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4 -0.014600  0.031418  0.090312 -0.000052 -0.013168 -0.069336  0.095244   \n",
       "5 -0.100688 -0.034349 -0.015562  0.102048 -0.034377 -0.029769 -0.074679   \n",
       "6 -0.015008  0.014014  0.043544 -0.039249  0.007010  0.022439  0.013127   \n",
       "7 -0.119047  0.027210  0.002545  0.006658 -0.011127  0.037048  0.062418   \n",
       "8 -0.021471  0.029862 -0.038230 -0.037036 -0.000850  0.018548  0.075258   \n",
       "9 -0.100688 -0.034349 -0.015562  0.102048 -0.034377 -0.029769 -0.074679   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "4 -0.012322 -0.057443 -0.061566    ...    -0.053096 -0.027628 -0.046655   \n",
       "5  0.027827  0.039846  0.077095    ...    -0.050505 -0.074465  0.032439   \n",
       "6 -0.034857  0.049377 -0.051198    ...     0.071647  0.033772  0.089638   \n",
       "7 -0.032071 -0.007870 -0.047811    ...     0.006505  0.060982 -0.125483   \n",
       "8  0.027883  0.055498  0.012769    ...     0.007132  0.028085 -0.053919   \n",
       "9  0.027827  0.039846  0.077095    ...    -0.050505 -0.074465  0.032439   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.011596  0.024088  0.015511 -0.029644  0.006272 -0.025874 -0.076605  \n",
       "5 -0.052896  0.089773 -0.094414 -0.032414  0.082769 -0.039997  0.041615  \n",
       "6  0.014955  0.064228  0.010969 -0.009657 -0.045884  0.019352  0.165582  \n",
       "7  0.103325  0.026611  0.054767 -0.005485 -0.085803  0.017624 -0.100847  \n",
       "8  0.025820  0.005494  0.030930  0.052201  0.046751  0.104778  0.004859  \n",
       "9 -0.052896  0.089773 -0.094414 -0.032414  0.082769 -0.039997  0.041615  \n",
       "\n",
       "[10 rows x 300 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = pd.DataFrame(train_image[1])\n",
    "print(p2.shape)\n",
    "p2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1620"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_image = torch.FloatTensor(train_image).reshape(-1, 1, max_l + 2 * (5-1), 300)\n",
    "t_label = torch.LongTensor(train_label)\n",
    "train_dataset = list(zip(t_image, t_label))\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: torch.Size([50, 1, 108, 300]) \n",
      "labels: torch.Size([50])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print('images:', images.shape, '\\nlabels:', labels.shape)\n",
    "    print(images[1][0])\n",
    "    print(labels[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_image = torch.FloatTensor(test_image).reshape(-1, 1, max_l + 2 * (5-1), 300)\n",
    "c_label = torch.LongTensor(test_label)\n",
    "test_dataset = list(zip(c_image, c_label, test_rev))\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters (original code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_hs=[3, 4, 5]\n",
    "hidden_units=[100, num_classes]\n",
    "batch_size=50\n",
    "img_w=300\n",
    "img_h = len(datasets[0][0])-1  # sentence length (subtracted 1 for y label)\n",
    "\n",
    "filter_w = img_w    \n",
    "feature_maps = hidden_units[0]\n",
    "filter_shapes = []\n",
    "pool_sizes = []\n",
    "for filter_h in filter_hs:\n",
    "    filter_shapes.append((feature_maps, 1, filter_h, filter_w))\n",
    "    pool_sizes.append((img_h-filter_h+1, img_w-filter_w+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]\n",
      "one batch train (50, 1, 108, 300)\n",
      "[(106, 1), (105, 1), (104, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(filter_shapes)\n",
    "image_shape = (batch_size, 1, img_h, img_w)\n",
    "print('one batch train', image_shape)\n",
    "print(pool_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding & Stride\n",
    "- Output size\n",
    "\n",
    "    $ O = \\frac {W-K+2P}{S} + 1 $\n",
    "    - O: output h/w\n",
    "    - W: input h/w\n",
    "    - K: filter size(kernel size)\n",
    "    - P: padding\n",
    "        - $  P = \\frac {K-1}{2} $\n",
    "    - S: stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "- Theano:\n",
    "    - conv_layer: LeNetConvPoolLayer\n",
    "    - classifier: MLPDropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:\n",
    "\n",
    "\n",
    "```\n",
    "Network\n",
    "Input ->\n",
    "Conv -> ReLU -> MaxPool |\n",
    "Conv -> ReLU -> MaxPool | -> concat\n",
    "Conv -> ReLU -> MaxPool |\n",
    "Fully Connected Layer(Logits -> Softmax) -> Labels\n",
    "```\n",
    "\n",
    "```\n",
    "Convolutional layer formula:\n",
    "- Filter 1(Kernel) size K = 3 => (3 x 300)\n",
    "- P(same padding) P = (3-1)/2=1\n",
    "- S(stride) S = 1\n",
    "- in_channels = 1\n",
    "- out_channels (int) – Number of channels produced by the convolution = 100\n",
    "Pooling layer formula:\n",
    "- K\n",
    "```\n",
    "\n",
    "```\n",
    "*Filter dimensions*:\n",
    "Conv1 (W_conv, (100, 1, 3, 300))\n",
    "Conv1 (b_conv, (100,))\n",
    "Conv2 (W_conv, (100, 1, 4, 300))\n",
    "Conv2 (b_conv, (100,))\n",
    "Conv3 (W_conv, (100, 1, 5, 300))\n",
    "Conv3 (b_conv, (100,))\n",
    "\n",
    "*Layer input dimensions*:\n",
    "- Input image(64, 300) \n",
    "\n",
    "----------------------------------------------------------------------\n",
    "|  Conv1  (100, 3, 300)   Conv2  (100, 4, 300)   Conv3  (100, 5, 300) |\n",
    "|  MaxPool (100, 62, 1)   MaxPool (100, 61, 1)   MaxPool (100, 60, 1) |\n",
    "-----------------------------Concat ----------------------------------\n",
    "\n",
    "- Concatenated (100, 1, 1) + (100, 1, 1) + (100, 1, 1) => (300, 1, 1) \n",
    "\n",
    "- Fully Connected Layer(Logits (100, 1) -> Logits (2, 1) -> Softmax) -> Labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPoolLayer(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvPoolLayer, self).__init__()\n",
    "\n",
    "        # Layer 1: conv - relu - conv- relu - pool\n",
    "        self.ngram1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=feature_maps, kernel_size=(filter_hs[0], img_w), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_sizes[0], stride=None))\n",
    "        self.ngram2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=feature_maps, kernel_size=(filter_hs[1], img_w), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_sizes[1], stride=None))\n",
    "        self.ngram3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=feature_maps, kernel_size=(filter_hs[2], img_w), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_sizes[2], stride=None))\n",
    "        \n",
    "        # Fully Connected 1 (readout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feature_maps * 3, hidden_units[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_units[0], num_classes))\n",
    "\n",
    "        # Initialize all parameters using kaiming normalization\n",
    "        self.init_weights_kaiming()\n",
    "    \n",
    "    def init_weights_kaiming(self):\n",
    "        #Use kaiming normalization to initialize the parameters\n",
    "        for layer in [self.ngram1, self.ngram2, self.ngram3, self.fc]:\n",
    "            for m in layer:\n",
    "                if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "                    m.weight = nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out1 = self.ngram1(x)\n",
    "        out2 = self.ngram2(x)\n",
    "        out3 = self.ngram3(x)\n",
    "        out = torch.cat((out1, out2, out3), 1)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "    \n",
    "        # Linear function (readout)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvPoolLayer(\n",
      "  (ngram1): Sequential(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(106, 1), stride=(106, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (ngram2): Sequential(\n",
      "    (0): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(105, 1), stride=(105, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (ngram3): Sequential(\n",
      "    (0): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(104, 1), stride=(104, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvPoolLayer(num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 3, 300])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1, 4, 300])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1, 5, 300])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 300])\n",
      "torch.Size([100])\n",
      "torch.Size([3, 100])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50. Loss: 0.20684829354286194.\n",
      "Iteration: 100. Loss: 0.011857990175485611.\n",
      "Iteration: 150. Loss: 0.007408613339066505.\n",
      "Iteration: 200. Loss: 0.0022772180382162333.\n",
      "Iteration: 250. Loss: 0.0014816458569839597.\n",
      "Iteration: 300. Loss: 0.0012197201140224934.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = list()\n",
    "iter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        total_loss = list()\n",
    "        \n",
    "        # Load images as Variable\n",
    "        images = Variable(images) # Now we dont need to resize like images.view(xx)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: Softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t paramters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Track loss to plot the result\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 50 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}.'.format(iter, loss.item()))\n",
    "            avg_loss = np.divide(np.sum(total_loss), len(total_loss))\n",
    "            avg_losses.append(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss vs iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = np.arange(len(avg_losses))\n",
    "plt.plot(x_axis, avg_losses, label='train')\n",
    "plt.xlabel('iterations (x100)')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 200 test images: 99.34210526315789 %\n"
     ]
    }
   ],
   "source": [
    "n_test = len(test_loader) * batch_size\n",
    "wrong_predictions = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels, revs in test_loader:\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # See which are error predictions\n",
    "        result = (predicted == labels)\n",
    "        err_imgs = images[result == 0] # 0 means wrong prediction\n",
    "        err_labels = labels[result == 0]\n",
    "        err_p = outputs[result == 0]\n",
    "        err_outputs = predicted[result == 0]\n",
    "        err_texts = np.array(revs['text'])[np.array((result == 0).numpy(), dtype=np.bool)]\n",
    "        for img, lbl, p, out, text in zip(err_imgs, err_labels, err_p, err_outputs, err_texts):\n",
    "            wrong_predictions.append((img, lbl, p, out, text))\n",
    "     \n",
    "    print('Test Accuracy of the model on the {} test images: {} %'.format(n_test, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label:  2 \t 暮らし系\n",
      "Predition:  0 \t エンタメ系\n",
      "Possibility: tensor([ 4.0938, -2.3852, -1.6469])\n",
      "だいたい ３ ０ ０ ０ 円 前後 な の か な ・ ・ ・ カード ゲーム 全然 やっ て ない から プレイマット って どういう 風 に 収納 し たり する の か わかっ て ない けど 折りたたん じゃう の か な ？ それとも 巻物 みたい に クルクル 巻く の か な ・ ・ ・ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_names = ['エンタメ系', '美容系', '暮らし系']\n",
    "# unpack img, lbl, out, text\n",
    "for img, lbl, p, out, text in wrong_predictions:\n",
    "    print('True label: ', lbl.item(), '\\t', label_names[lbl.item()])\n",
    "    print('Predition: ', out.item(), '\\t', label_names[out.item()])\n",
    "    print('Possibility:', p)\n",
    "    print(text, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1650 train images: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "n_train = len(train_loader) * batch_size\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Test Accuracy of the model on the {} train images: {} %'.format(n_train, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict single inputted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input the text to predict (change the text below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_to_predict = \"お風呂掃除でいつも落ちなかった溝にある黒カビが家事えもんの塩素系漂白剤＋片栗粉でほぼ真っ白になって感動 家事えもんのテクニック凄い～！\"\n",
    "\n",
    "text_to_predict = \"コスメの最安値が見つけられるアプリ💄💋メイク動画とか 美容情報も載ってるし最高😆🙌📲http://goo.gl/K5Fmea 女子にはほんとに助かる〜💗\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "data loaded!\n",
      "number of sentences: 1772\n",
      "vocab size: 9467\n",
      "max sentence length: 100\n",
      "loading word2vec vectors...\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from process_data import build_single_data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_make_idx_data_cv_2vec(revs, U, word_idx_map, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    test_image, test_label = [], []\n",
    "    test_rev = []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent_2vec(rev[\"text\"], U, word_idx_map, max_l, k, filter_h) # one sentence\n",
    "        test_image.append(sent) \n",
    "        test_label.append(rev[\"y\"])\n",
    "        test_rev.append(rev)\n",
    "\n",
    "    test_image = np.array(test_image)\n",
    "    test_label = np.array(test_label)\n",
    "    test_rev = np.array(test_rev)\n",
    "    return test_image, test_label, test_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1行処理済み\n"
     ]
    }
   ],
   "source": [
    "single_revs, _ = build_single_data_cv(text_to_predict)\n",
    "single_data_2vec = single_make_idx_data_cv_2vec(single_revs, U, word_idx_map, max_l, k=300, filter_h=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1 美容系\n",
      "Text: コスメ の 最 安値 が 見つけ られる アプリメイク 動画 とか 美容 情報 も 載っ てる し 最高 女子 に は ほんとに 助かる 〜\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, labels, revs = single_data_2vec\n",
    "    images = Variable(torch.Tensor(images.reshape(1, 1, -1, 300)))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print('Prediction:', predicted.item(), label_names[predicted.item()])\n",
    "    print('Text:', revs[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
