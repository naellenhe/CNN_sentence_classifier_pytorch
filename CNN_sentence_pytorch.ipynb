{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cPickle (python2.7)\n",
    "#http://testpy.hatenablog.com/entry/2017/03/17/000626\n",
    "import _pickle as cPickle\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pickle file contains [revs, W, W2, word_idx_map, vocab]\n",
    "x = cPickle.load(open(\"mr.p\",\"rb\"), encoding=\"latin1\") # Add encoding=\"latin1\" because got UnicodeDecodeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs, W, W2, word_idx_map, vocab = x[0], x[1], x[2], x[3], x[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  56\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "max_l = np.max(pd.DataFrame(revs)[\"num_words\"])\n",
    "print(\"max sentence length: \", max_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revs 10662\n",
      "W 18766\n",
      "W2 18766\n",
      "word_idx_map 18765\n",
      "vocab 18765\n"
     ]
    }
   ],
   "source": [
    "print('revs',len(x[0])) # number of sentence\n",
    "print('W', len(x[1]))\n",
    "print('W2', len(x[2])) # W2 are randomly initialized vectors\n",
    "print('word_idx_map', len(x[3]))\n",
    "print('vocab', len(x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 1, 'text': 'effective but too tepid biopic', 'split': 7, 'num_words': 5}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12002"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx_map['good'] # word and its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['good'] # word and its count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model architecture: CNN-non-static\n",
      "using: random vectors\n"
     ]
    }
   ],
   "source": [
    "# mode= sys.argv[1]\n",
    "# word_vectors = sys.argv[2]\n",
    "\n",
    "mode = \"-nonstatic\"\n",
    "word_vectors = \"-rand\"\n",
    "\n",
    "if mode==\"-nonstatic\":\n",
    "    print(\"model architecture: CNN-non-static\")\n",
    "    non_static=True\n",
    "elif mode==\"-static\":\n",
    "    print(\"model architecture: CNN-static\")\n",
    "    non_static=False\n",
    "\n",
    "#execfile(\"conv_net_classes.py\")  \n",
    "\n",
    "if word_vectors==\"-rand\":\n",
    "    print(\"using: random vectors\")\n",
    "    U = W2\n",
    "elif word_vectors==\"-word2vec\":\n",
    "    print(\"using: word2vec vectors\")\n",
    "    U = W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset\n",
    "make each sentence an word index map using word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    pad = filter_h - 1\n",
    "    for i in range(pad):\n",
    "        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l+2*pad:\n",
    "        x.append(0)\n",
    "    return x\n",
    "\n",
    "def make_idx_data_cv(revs, word_idx_map, cv, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, test = [], []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k, filter_h) # one sentence\n",
    "        sent.append(rev[\"y\"])\n",
    "        if rev[\"split\"]==cv:  # \"split\" is random number of np.random.randint(0,10)\n",
    "            test.append(sent)        \n",
    "        else:  \n",
    "            train.append(sent)   \n",
    "    train = np.array(train, dtype=\"int\")\n",
    "    test = np.array(test, dtype=\"int\")\n",
    "    return [train, test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence length(before) 64\n",
      "sentence length(after added y label) 65\n"
     ]
    }
   ],
   "source": [
    "t = \"effective but too tepid biopic\"\n",
    "t_sent = get_idx_from_sent(t, word_idx_map, max_l, k=300, filter_h=5)\n",
    "print(\"sentence length(before)\", len(t_sent)) # max_l(51)+2*pad(filter_h-1)\n",
    "t_sent.append(1) #sent.append(rev[\"y\"])\n",
    "print(\"sentence length(after added y label)\", len(t_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "datasets = make_idx_data_cv(revs, word_idx_map, i, max_l=56, k=300, filter_h=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,  5563, 10855, 10100,   262, 13764,\n",
       "        3291,  5563, 10487, 12491,  6797,  9380,  9224, 16503,  6347,\n",
       "        9195,  6797,  1773, 13764, 10123,  7252, 11843,  9366, 13213,\n",
       "         599, 16044, 11417,  9596, 14903,  1356, 18642,  8580, 16278,\n",
       "       12741,  2064,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(datasets[0][0]))\n",
    "datasets[0][0] # sentence => word index map padding with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: (9572, 65)\n",
      "test data size: (1090, 65)\n"
     ]
    }
   ],
   "source": [
    "print('train data size:', datasets[0].shape)\n",
    "print('test data size:', datasets[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('image shape', 64, 300), ('filter shape', [(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]), ('hidden_units', [100, 2]), ('dropout', [0.5]), ('batch_size', 50), ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]\n"
     ]
    }
   ],
   "source": [
    "lr_decay=0.95\n",
    "filter_hs=[3,4,5]\n",
    "conv_non_linear=\"relu\"\n",
    "hidden_units=[100,2]\n",
    "shuffle_batch=True\n",
    "n_epochs=25\n",
    "sqr_norm_lim=9\n",
    "non_static=non_static\n",
    "batch_size=50\n",
    "dropout_rate=[0.5]\n",
    "\n",
    "img_w=300\n",
    "shuffle_batch=True\n",
    "# activations=[Iden]\n",
    "\n",
    "\"\"\"\n",
    "Train a simple conv net\n",
    "img_h = sentence length (padded where necessary)\n",
    "img_w = word vector length (300 for word2vec)\n",
    "filter_hs = filter window sizes    \n",
    "hidden_units = [x,y] x is the number of feature maps (per filter window), and y is the penultimate layer\n",
    "sqr_norm_lim = s^2 in the paper\n",
    "lr_decay = adadelta decay parameter\n",
    "\"\"\"    \n",
    "rng = np.random.RandomState(3435)\n",
    "img_h = len(datasets[0][0])-1  # sentence length (subtracted 1 for y label)\n",
    "filter_w = img_w    \n",
    "feature_maps = hidden_units[0]\n",
    "filter_shapes = []\n",
    "pool_sizes = []\n",
    "for filter_h in filter_hs:\n",
    "    filter_shapes.append((feature_maps, 1, filter_h, filter_w))\n",
    "    pool_sizes.append((img_h-filter_h+1, img_w-filter_w+1))\n",
    "\n",
    "# filter_shapes [(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]\n",
    "# pool_sizes [(62, 1), (61, 1), (60, 1)]\n",
    "\n",
    "parameters = [(\"image shape\",img_h,img_w),(\"filter shape\",filter_shapes), (\"hidden_units\",hidden_units),\n",
    "              (\"dropout\", dropout_rate), (\"batch_size\",batch_size),(\"non_static\", non_static),\n",
    "                (\"learn_decay\",lr_decay), (\"conv_non_linear\", conv_non_linear), (\"non_static\", non_static)\n",
    "                ,(\"sqr_norm_lim\",sqr_norm_lim),(\"shuffle_batch\",shuffle_batch)]\n",
    "print(parameters)   \n",
    "\n",
    "# [('image shape', 64, 300), ('filter shape', [(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]), \n",
    "# ('hidden_units', [100, 2]), ('dropout', [0.5]), ('batch_size', 50), \n",
    "# ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), \n",
    "# ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]\n",
      "one batch (50, 1, 64, 300)\n",
      "[(62, 1), (61, 1), (60, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(filter_shapes)\n",
    "image_shape=(batch_size, 1, img_h, img_w)\n",
    "print('one batch', image_shape)\n",
    "print(pool_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "- Theano:\n",
    "    - conv_layer: LeNetConvPoolLayer\n",
    "    - classifier: MLPDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Train/Val/Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle dataset and assign to mini batches. if dataset size is not a multiple of mini batches, replicate \n",
    "#extra data (at random)\n",
    "np.random.seed(3435)\n",
    "if datasets[0].shape[0] % batch_size > 0:\n",
    "    extra_data_num = batch_size - datasets[0].shape[0] % batch_size\n",
    "    train_set = np.random.permutation(datasets[0])   \n",
    "    extra_data = train_set[:extra_data_num]\n",
    "    new_data=np.append(datasets[0],extra_data,axis=0)\n",
    "else:\n",
    "    new_data = datasets[0]\n",
    "new_data = np.random.permutation(new_data)\n",
    "\n",
    "# datasets[0].shape (9572, 65)\n",
    "# new_data.shape (9600, 65) (dividable by batch_size)\n",
    "\n",
    "n_batches = new_data.shape[0]/batch_size\n",
    "n_train_batches = int(np.round(n_batches*0.9)) # 90% for train, 10% for val\n",
    "\n",
    "#divide train set into train/val sets \n",
    "test_set_x = datasets[1][:,:img_h] \n",
    "test_set_y = np.asarray(datasets[1][:,-1], \"int32\")\n",
    "train_set = new_data[:n_train_batches * batch_size,:]\n",
    "val_set = new_data[n_train_batches*batch_size:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "#r = range(0, 10) # orginal code\n",
    "r = range(0, 1) \n",
    "for i in r:\n",
    "    datasets = make_idx_data_cv(revs, word_idx_map, i, max_l=56,k=300, filter_h=5)\n",
    "    perf = train_conv_net(datasets,\n",
    "                          U,\n",
    "                          lr_decay=0.95,\n",
    "                          filter_hs=[3,4,5],\n",
    "                          conv_non_linear=\"relu\",\n",
    "                          hidden_units=[100,2], \n",
    "                          shuffle_batch=True, \n",
    "                          n_epochs=25, \n",
    "                          sqr_norm_lim=9,\n",
    "                          non_static=non_static,\n",
    "                          batch_size=50,\n",
    "                          dropout_rate=[0.5])\n",
    "    print \"cv: \" + str(i) + \", perf: \" + str(perf)\n",
    "    results.append(perf)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
