{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import sys, re\n",
    "import pandas as pd\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary dictionary : word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_cv(data_folder, cv=10):\n",
    "    \"\"\"\n",
    "    Loads data and split into 10 folds.\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    label_0_file, label_1_file, label_2_file = data_folder #[\"__label__0.txt\",\"__label__1.txt\", \"__label__2.txt\"]    \n",
    "    word_idx_map = dict()\n",
    "    word_idx = 1\n",
    "    for label, file in enumerate(data_folder): \n",
    "        with codecs.open(file, \"rb\", \"utf-8\", \"ignore\") as f:\n",
    "            for line in f:       \n",
    "                rev = []\n",
    "                rev.append(line.strip())\n",
    "                orig_rev = \" \".join(rev).lower()\n",
    "\n",
    "                words = set(orig_rev.split())\n",
    "                for word in words:\n",
    "                    if word not in word_idx_map:\n",
    "                        word_idx_map[word] = word_idx\n",
    "                        word_idx += 1\n",
    "\n",
    "                datum  = {\"y\": label, \n",
    "                          \"text\": orig_rev,                             \n",
    "                          \"num_words\": len(orig_rev.split()),\n",
    "                          \"split\": np.random.randint(0, cv)}\n",
    "                revs.append(datum)\n",
    "    return revs, word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "data_folder = [\"__label__0.txt\",\"__label__1.txt\", \"__label__2.txt\"]    \n",
    "\n",
    "print(\"loading data...\")      \n",
    "revs, word_idx_map = build_data_cv(data_folder, cv=10)\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset content\n",
    "Get data from twitter (reference: http://tech.wonderpla.net/entry/2017/10/10/110000)\n",
    "- Label 0\n",
    "    - KEYWORD = \"芸能 OR アニメ OR 漫画 OR ドラマ OR ゲーム\"            #エンタメ系のキーワード\n",
    "    - CLASS_LABEL = \"\\__label__0\"\n",
    "\n",
    "- Label 1\n",
    "    - KEYWORD = \"美容 OR サロン OR エステ OR 化粧 OR 保湿\"            #美容系のキーワード\n",
    "    - CLASS_LABEL = \"\\__label__1\"\n",
    "\n",
    "- Label 2\n",
    "    - KEYWORD = \"日常 OR 料理 OR 家事 OR 収納 OR 家具\"            #暮らし系のキーワード\n",
    "    - CLASS_LABEL = \"\\__label__2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['エンタメ系', '美容系', '暮らし系']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td># ピザ ハット 39 ルーレット に ご 応募 ありがとう ござい ます !(* ⁰ ▿ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td># ピザ ハット 39 ルーレット に ご 応募 ありがとう ござい ます !(* ⁰ ▿ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_words  split                                               text  y\n",
       "108         60      7  # ピザ ハット 39 ルーレット に ご 応募 ありがとう ござい ます !(* ⁰ ▿ ...  0\n",
       "260         60      4  # ピザ ハット 39 ルーレット に ご 応募 ありがとう ござい ます !(* ⁰ ▿ ...  0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(revs)\n",
    "\n",
    "# label 0: Entertainment \n",
    "df[df['y'] == 0].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>普段 濃い 化粧 全然 し ない から 無駄 に 自 撮り し て た やつ な dc お ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>今日 は 幼稚園 の ママ の ランチ 会 な ん だ けど 、 化粧 す べき か どうか...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_words  split                                               text  y\n",
       "909          31      5  普段 濃い 化粧 全然 し ない から 無駄 に 自 撮り し て た やつ な dc お ...  1\n",
       "1159         39      0  今日 は 幼稚園 の ママ の ランチ 会 な ん だ けど 、 化粧 す べき か どうか...  1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label 1 : Beauty\n",
    "df[df['y'] == 1].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>車 じゃ ない と なかなか 行け ない の です が 、 地元 の 海鮮 と タイ 料理 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>《 8 / 30 newopen 》 ドリンク 総数 約 １ ０ ０ 種類 ♪ ワイン や ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_words  split                                               text  y\n",
       "1464         28      1  車 じゃ ない と なかなか 行け ない の です が 、 地元 の 海鮮 と タイ 料理 ...  2\n",
       "1835         55      8  《 8 / 30 newopen 》 ドリンク 総数 約 １ ０ ０ 種類 ♪ ワイン や ...  2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label 2: Life\n",
    "df[df['y'] == 2].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revs 2068\n",
      "word_idx_map 10329\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_idx_map)\n",
    "print('revs',len(revs)) # number of sentence\n",
    "print('word_idx_map', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 0,\n",
       " 'text': 'え 、 サラリーマン し ながら 、 商業 漫画 の 仕事 を し て 、 ツイッター に 定期 的 に 漫画 を あげ て 、 さらに コミケ の 原稿 を 作る ！ ？',\n",
       " 'num_words': 32,\n",
       " 'split': 6}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make BOW vectors\n",
    "make each sentence an BOW vector using word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_from_sent(revs, word_idx_map, cv, vocab_size):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, test = [], []\n",
    "    for rev in revs:\n",
    "        sent = rev[\"text\"]\n",
    "        bow_vec = np.zeros(vocab_size + 1)\n",
    "        bow_vec[0] = rev['y'] # Add true label at the beginning\n",
    "        words = sent.split()\n",
    "        for word in words:\n",
    "            bow_vec[word_idx_map[word]] += 1\n",
    "\n",
    "        if rev[\"split\"]== cv:  # \"split\" is random number of np.random.randint(0,10)\n",
    "            test.append(bow_vec)        \n",
    "        else:  \n",
    "            train.append(bow_vec)   \n",
    "    train = np.array(train, dtype=\"int\")\n",
    "    test = np.array(test, dtype=\"int\")\n",
    "    return [train, test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 0\n",
    "datasets = make_bow_from_sent(revs, word_idx_map, cv, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of sentence to BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 0,\n",
       " 'text': 'かっこいい 、 わくわく する 漫画 も 気 に なっ てる けど なかなか 読め て ない ん だ よ ねぇ 、 満喫 行こ う か な と 思っ て も 家 で ゴロゴロ っと し ちゃう',\n",
       " 'num_words': 35,\n",
       " 'split': 2}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = datasets[0][:, 1:]\n",
    "train_label = datasets[0][:, 0]\n",
    "test_image = datasets[1][:, 1:]\n",
    "test_label = datasets[1][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: (1857, 10329)\n",
      "train data size: (1857,)\n",
      "test data size: (211, 10329)\n",
      "train data size: (211,)\n"
     ]
    }
   ],
   "source": [
    "print('train data size:', train_image.shape)\n",
    "print('train data size:', train_label.shape)\n",
    "print('test data size:', test_image.shape)\n",
    "print('train data size:', test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_image, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.990521327014\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', clf.score(test_image, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996768982229\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', clf.score(train_image, train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict single inputted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input the text to predict (change the text below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_to_predict = \"お風呂掃除でいつも落ちなかった溝にある黒カビが家事えもんの塩素系漂白剤＋片栗粉でほぼ真っ白になって感動 家事えもんのテクニック凄い～！\"\n",
    "\n",
    "text_to_predict = \"コスメの最安値が見つけられるアプリ💄💋メイク動画とか 美容情報も載ってるし最高😆🙌📲http://goo.gl/K5Fmea 女子にはほんとに助かる〜💗\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_data import process_single_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_from_single_sent(sent, word_idx_map, vocab_size):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    test = []\n",
    "    bow_vec = np.zeros(vocab_size + 1)\n",
    "    bow_vec[0] = 0 # Add dummy true label at the beginning\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            bow_vec[word_idx_map[word]] += 1\n",
    "\n",
    "    test.append(bow_vec)\n",
    "    test = np.array(test, dtype=\"int\")\n",
    "    return test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1行処理済み\n"
     ]
    }
   ],
   "source": [
    "processed_sent = process_single_txt(text_to_predict)\n",
    "single_bow = make_bow_from_single_sent(processed_sent, word_idx_map, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1 美容系\n"
     ]
    }
   ],
   "source": [
    "predected = clf.predict(single_bow[:, 1:]).item()\n",
    "print('Predicted:', predected, label_names[predected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
