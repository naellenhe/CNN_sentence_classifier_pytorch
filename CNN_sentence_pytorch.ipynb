{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cPickle (python2.7)\n",
    "#http://testpy.hatenablog.com/entry/2017/03/17/000626\n",
    "import _pickle as cPickle\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pickle file contains [revs, W, W2, word_idx_map, vocab]\n",
    "x = cPickle.load(open(\"mr.p\",\"rb\"), encoding=\"latin1\") # Add encoding=\"latin1\" because got UnicodeDecodeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "revs, W, W2, word_idx_map, vocab = x[0], x[1], x[2], x[3], x[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  56\n"
     ]
    }
   ],
   "source": [
    "max_l = np.max(pd.DataFrame(revs)[\"num_words\"])\n",
    "print(\"max sentence length: \", max_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revs 10662\n",
      "W 18766\n",
      "W2 18766\n",
      "word_idx_map 18765\n",
      "vocab 18765\n"
     ]
    }
   ],
   "source": [
    "print('revs',len(x[0])) # number of sentence\n",
    "print('W', len(x[1]))\n",
    "print('W2', len(x[2])) # W2 are randomly initialized vectors\n",
    "print('word_idx_map', len(x[3]))\n",
    "print('vocab', len(x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 1, 'text': 'effective but too tepid biopic', 'split': 7, 'num_words': 5}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12002"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx_map['good'] # word and its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['good'] # word and its count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model architecture: CNN-non-static\n",
      "using: random vectors\n"
     ]
    }
   ],
   "source": [
    "# mode= sys.argv[1]\n",
    "# word_vectors = sys.argv[2]\n",
    "\n",
    "mode = \"-nonstatic\"\n",
    "word_vectors = \"-rand\"\n",
    "\n",
    "if mode==\"-nonstatic\":\n",
    "    print(\"model architecture: CNN-non-static\")\n",
    "    non_static=True\n",
    "elif mode==\"-static\":\n",
    "    print(\"model architecture: CNN-static\")\n",
    "    non_static=False\n",
    "\n",
    "#execfile(\"conv_net_classes.py\")  \n",
    "\n",
    "if word_vectors==\"-rand\":\n",
    "    print(\"using: random vectors\")\n",
    "    U = W2\n",
    "elif word_vectors==\"-word2vec\":\n",
    "    print(\"using: word2vec vectors\")\n",
    "    U = W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18766, 300)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset (check original code)\n",
    "make each sentence an word index map using word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_from_sent(sent, word_idx_map, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    x = []\n",
    "    pad = filter_h - 1\n",
    "    for i in range(pad):\n",
    "        x.append(0)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x.append(word_idx_map[word])\n",
    "    while len(x) < max_l+2*pad:\n",
    "        x.append(0)\n",
    "    return x\n",
    "\n",
    "def make_idx_data_cv(revs, word_idx_map, cv, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, test = [], []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent(rev[\"text\"], word_idx_map, max_l, k, filter_h) # one sentence\n",
    "        sent.append(rev[\"y\"])\n",
    "        if rev[\"split\"]==cv:  # \"split\" is random number of np.random.randint(0,10)\n",
    "            test.append(sent)        \n",
    "        else:  \n",
    "            train.append(sent)   \n",
    "    train = np.array(train, dtype=\"int\")\n",
    "    test = np.array(test, dtype=\"int\")\n",
    "    return [train, test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence length(before) 64\n",
      "sentence length(after added y label) 65\n"
     ]
    }
   ],
   "source": [
    "t = \"effective but too tepid biopic\"\n",
    "t_sent = get_idx_from_sent(t, word_idx_map, max_l, k=300, filter_h=5)\n",
    "print(\"sentence length(before)\", len(t_sent)) # max_l(51)+2*pad(filter_h-1)\n",
    "t_sent.append(1) #sent.append(rev[\"y\"])\n",
    "print(\"sentence length(after added y label)\", len(t_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "datasets = make_idx_data_cv(revs, word_idx_map, i, max_l=56, k=300, filter_h=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of sentence to word index map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 1, 'text': 'effective but too tepid biopic', 'split': 7, 'num_words': 5}"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,   271, 13936, 14497, 17972,  5678,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     1])"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(datasets[0][0]))\n",
    "datasets[0][2] # sentence => word index map padding with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: (9572, 65)\n",
      "test data size: (1090, 65)\n"
     ]
    }
   ],
   "source": [
    "print('train data size:', datasets[0].shape)\n",
    "print('test data size:', datasets[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset (using vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_from_sent_2vec(sent, U, word_idx_map, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentence into a list of indices. Pad with zeroes.\n",
    "    \"\"\"\n",
    "    pad = filter_h - 1\n",
    "    x = np.zeros((max_l+2*pad, k))\n",
    "\n",
    "    words = sent.split()\n",
    "    # starting after padding\n",
    "    i = pad\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            x[i] = U[word_idx_map[word]]\n",
    "            i += 1\n",
    "    return x\n",
    "\n",
    "def make_idx_data_cv_2vec(revs, U, word_idx_map, cv, max_l=51, k=300, filter_h=5):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train_image, train_label = [], []\n",
    "    test_image, test_label = [], []\n",
    "    for rev in revs:\n",
    "        sent = get_idx_from_sent_2vec(rev[\"text\"], U, word_idx_map, max_l, k, filter_h) # one sentence\n",
    "        if rev[\"split\"]==cv:  # \"split\" is random number of np.random.randint(0,10)\n",
    "            test_image.append(sent) \n",
    "            test_label.append(rev[\"y\"])\n",
    "        else:  \n",
    "            train_image.append(sent)\n",
    "            train_label.append(rev[\"y\"])\n",
    "    train_image = np.array(train_image)\n",
    "    train_label = np.array(train_label)\n",
    "    test_image = np.array(test_image)\n",
    "    test_label = np.array(test_label)\n",
    "    return (train_image, train_label), (test_image, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence length(before) 64\n"
     ]
    }
   ],
   "source": [
    "t = \"effective but too tepid biopic\"\n",
    "t_sent_2vec = get_idx_from_sent_2vec(t, W, word_idx_map, max_l, k=300, filter_h=5)\n",
    "print(\"sentence length(before)\", len(t_sent_2vec)) # max_l(51)+2*pad(filter_h-1)\n",
    "# t_sent_2vec.append(1) #sent.append(rev[\"y\"])\n",
    "# print(\"sentence length(after added y label)\", len(t_sent_2vec ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_sent_2vec[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028931</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.125977</td>\n",
       "      <td>0.078613</td>\n",
       "      <td>-0.182617</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>-0.005280</td>\n",
       "      <td>0.308594</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177734</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.070801</td>\n",
       "      <td>-0.003143</td>\n",
       "      <td>-0.104980</td>\n",
       "      <td>0.339844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.047607</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.045654</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114746</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>0.092285</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.049316</td>\n",
       "      <td>-0.055664</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>-0.108398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.129883</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>-0.032959</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>-0.138672</td>\n",
       "      <td>0.141602</td>\n",
       "      <td>0.192383</td>\n",
       "      <td>-0.053955</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.070312</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>-0.137695</td>\n",
       "      <td>-0.045166</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>0.032959</td>\n",
       "      <td>0.208984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.248047</td>\n",
       "      <td>0.236328</td>\n",
       "      <td>0.107422</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.257812</td>\n",
       "      <td>-0.014771</td>\n",
       "      <td>-0.118164</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057861</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>-0.055176</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>-0.069336</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.175781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.285156</td>\n",
       "      <td>-0.195312</td>\n",
       "      <td>0.109863</td>\n",
       "      <td>0.237305</td>\n",
       "      <td>0.322266</td>\n",
       "      <td>0.363281</td>\n",
       "      <td>0.015869</td>\n",
       "      <td>-0.137695</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>-0.427734</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076660</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>-0.365234</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>0.024780</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.109863</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>0.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4 -0.028931 -0.034912 -0.125977  0.078613 -0.182617  0.080078  0.143555   \n",
       "5 -0.047607  0.081543  0.045654  0.091797 -0.014709  0.111328  0.065430   \n",
       "6  0.129883  0.131836 -0.032959  0.148438 -0.138672  0.141602  0.192383   \n",
       "7  0.248047  0.236328  0.107422  0.217773 -0.257812 -0.014771 -0.118164   \n",
       "8  0.285156 -0.195312  0.109863  0.237305  0.322266  0.363281  0.015869   \n",
       "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "4 -0.005280  0.308594  0.070801    ...    -0.177734 -0.082520  0.108398   \n",
       "5 -0.096680  0.138672  0.143555    ...    -0.114746  0.041504 -0.041992   \n",
       "6 -0.053955  0.110352  0.068848    ...     0.006165  0.079102 -0.070312   \n",
       "7 -0.064941  0.076172  0.291016    ...    -0.057861 -0.306641 -0.055176   \n",
       "8 -0.137695  0.017334 -0.427734    ...    -0.076660  0.111328 -0.365234   \n",
       "9  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.208008 -0.145508  0.375000 -0.070801 -0.003143 -0.104980  0.339844  \n",
       "5  0.092285 -0.000713  0.075195  0.049316 -0.055664  0.104980 -0.108398  \n",
       "6  0.025757 -0.137695 -0.045166  0.070801 -0.065918  0.032959  0.208984  \n",
       "7  0.128906  0.067383 -0.069336 -0.001877  0.133789  0.057617  0.175781  \n",
       "8  0.025757  0.011292  0.024780  0.001472  0.109863 -0.053467  0.296875  \n",
       "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[10 rows x 300 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pd.DataFrame(t_sent_2vec[:64])\n",
    "p.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "datasets_2vec = make_idx_data_cv_2vec(revs, W, word_idx_map, i, max_l=56, k=300, filter_h=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_image, train_label), (test_image, test_label) = datasets_2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of sentence to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 1, 'text': 'effective but too tepid biopic', 'split': 7, 'num_words': 5}"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 300)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.028931</td>\n",
       "      <td>-0.034912</td>\n",
       "      <td>-0.125977</td>\n",
       "      <td>0.078613</td>\n",
       "      <td>-0.182617</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>-0.005280</td>\n",
       "      <td>0.308594</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177734</td>\n",
       "      <td>-0.082520</td>\n",
       "      <td>0.108398</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>-0.070801</td>\n",
       "      <td>-0.003143</td>\n",
       "      <td>-0.104980</td>\n",
       "      <td>0.339844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.047607</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.045654</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>0.065430</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114746</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>-0.041992</td>\n",
       "      <td>0.092285</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>0.075195</td>\n",
       "      <td>0.049316</td>\n",
       "      <td>-0.055664</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>-0.108398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.129883</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>-0.032959</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>-0.138672</td>\n",
       "      <td>0.141602</td>\n",
       "      <td>0.192383</td>\n",
       "      <td>-0.053955</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.068848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>-0.070312</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>-0.137695</td>\n",
       "      <td>-0.045166</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>0.032959</td>\n",
       "      <td>0.208984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.248047</td>\n",
       "      <td>0.236328</td>\n",
       "      <td>0.107422</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.257812</td>\n",
       "      <td>-0.014771</td>\n",
       "      <td>-0.118164</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057861</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>-0.055176</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>-0.069336</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>0.057617</td>\n",
       "      <td>0.175781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.285156</td>\n",
       "      <td>-0.195312</td>\n",
       "      <td>0.109863</td>\n",
       "      <td>0.237305</td>\n",
       "      <td>0.322266</td>\n",
       "      <td>0.363281</td>\n",
       "      <td>0.015869</td>\n",
       "      <td>-0.137695</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>-0.427734</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076660</td>\n",
       "      <td>0.111328</td>\n",
       "      <td>-0.365234</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>0.024780</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.109863</td>\n",
       "      <td>-0.053467</td>\n",
       "      <td>0.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4 -0.028931 -0.034912 -0.125977  0.078613 -0.182617  0.080078  0.143555   \n",
       "5 -0.047607  0.081543  0.045654  0.091797 -0.014709  0.111328  0.065430   \n",
       "6  0.129883  0.131836 -0.032959  0.148438 -0.138672  0.141602  0.192383   \n",
       "7  0.248047  0.236328  0.107422  0.217773 -0.257812 -0.014771 -0.118164   \n",
       "8  0.285156 -0.195312  0.109863  0.237305  0.322266  0.363281  0.015869   \n",
       "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        7         8         9      ...          290       291       292  \\\n",
       "0  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "4 -0.005280  0.308594  0.070801    ...    -0.177734 -0.082520  0.108398   \n",
       "5 -0.096680  0.138672  0.143555    ...    -0.114746  0.041504 -0.041992   \n",
       "6 -0.053955  0.110352  0.068848    ...     0.006165  0.079102 -0.070312   \n",
       "7 -0.064941  0.076172  0.291016    ...    -0.057861 -0.306641 -0.055176   \n",
       "8 -0.137695  0.017334 -0.427734    ...    -0.076660  0.111328 -0.365234   \n",
       "9  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "\n",
       "        293       294       295       296       297       298       299  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.208008 -0.145508  0.375000 -0.070801 -0.003143 -0.104980  0.339844  \n",
       "5  0.092285 -0.000713  0.075195  0.049316 -0.055664  0.104980 -0.108398  \n",
       "6  0.025757 -0.137695 -0.045166  0.070801 -0.065918  0.032959  0.208984  \n",
       "7  0.128906  0.067383 -0.069336 -0.001877  0.133789  0.057617  0.175781  \n",
       "8  0.025757  0.011292  0.024780  0.001472  0.109863 -0.053467  0.296875  \n",
       "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[10 rows x 300 columns]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = pd.DataFrame(train_image[2])\n",
    "print(p2.shape)\n",
    "p2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9572"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_image = torch.FloatTensor(train_image).reshape(-1, 1, 64, 300)\n",
    "t_label = torch.LongTensor(train_label)\n",
    "train_dataset = list(zip(t_image, t_label))\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "batch_size = 50\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: torch.Size([50, 1, 64, 300]) \n",
      "labels: torch.Size([50])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print('images:', images.shape, '\\nlabels:', labels.shape)\n",
    "    print(images[1][0])\n",
    "    print(labels[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1090"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_image = torch.FloatTensor(test_image).reshape(-1, 1, 64, 300)\n",
    "c_label = torch.LongTensor(test_label)\n",
    "test_dataset = list(zip(c_image, c_label))\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "batch_size = 50\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: torch.Size([50, 1, 64, 300]) \n",
      "labels: torch.Size([50])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in test_loader:\n",
    "    print('images:', images.shape, '\\nlabels:', labels.shape)\n",
    "    print(images[1][0])\n",
    "    print(labels[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('image shape', 64, 300), ('filter shape', [(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]), ('hidden_units', [100, 2]), ('dropout', [0.5]), ('batch_size', 50), ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]\n"
     ]
    }
   ],
   "source": [
    "lr_decay=0.95\n",
    "filter_hs=[3,4,5]\n",
    "conv_non_linear=\"relu\"\n",
    "hidden_units=[100,2]\n",
    "shuffle_batch=True\n",
    "n_epochs=25\n",
    "sqr_norm_lim=9\n",
    "non_static=non_static\n",
    "batch_size=50\n",
    "dropout_rate=[0.5]\n",
    "\n",
    "img_w=300\n",
    "shuffle_batch=True\n",
    "# activations=[Iden]\n",
    "\n",
    "\"\"\"\n",
    "Train a simple conv net\n",
    "img_h = sentence length (padded where necessary)\n",
    "img_w = word vector length (300 for word2vec)\n",
    "filter_hs = filter window sizes    \n",
    "hidden_units = [x,y] x is the number of feature maps (per filter window), and y is the penultimate layer\n",
    "sqr_norm_lim = s^2 in the paper\n",
    "lr_decay = adadelta decay parameter\n",
    "\"\"\"    \n",
    "rng = np.random.RandomState(3435)\n",
    "img_h = len(datasets[0][0])-1  # sentence length (subtracted 1 for y label)\n",
    "filter_w = img_w    \n",
    "feature_maps = hidden_units[0]\n",
    "filter_shapes = []\n",
    "pool_sizes = []\n",
    "for filter_h in filter_hs:\n",
    "    filter_shapes.append((feature_maps, 1, filter_h, filter_w))\n",
    "    pool_sizes.append((img_h-filter_h+1, img_w-filter_w+1))\n",
    "\n",
    "# filter_shapes [(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]\n",
    "# pool_sizes [(62, 1), (61, 1), (60, 1)]\n",
    "\n",
    "parameters = [(\"image shape\",img_h,img_w),(\"filter shape\",filter_shapes), (\"hidden_units\",hidden_units),\n",
    "              (\"dropout\", dropout_rate), (\"batch_size\",batch_size),(\"non_static\", non_static),\n",
    "                (\"learn_decay\",lr_decay), (\"conv_non_linear\", conv_non_linear), (\"non_static\", non_static)\n",
    "                ,(\"sqr_norm_lim\",sqr_norm_lim),(\"shuffle_batch\",shuffle_batch)]\n",
    "print(parameters)   \n",
    "\n",
    "# [('image shape', 64, 300), ('filter shape', [(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]), \n",
    "# ('hidden_units', [100, 2]), ('dropout', [0.5]), ('batch_size', 50), \n",
    "# ('non_static', True), ('learn_decay', 0.95), ('conv_non_linear', 'relu'), \n",
    "# ('non_static', True), ('sqr_norm_lim', 9), ('shuffle_batch', True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 1, 3, 300), (100, 1, 4, 300), (100, 1, 5, 300)]\n",
      "one batch (50, 1, 64, 300)\n",
      "[(62, 1), (61, 1), (60, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(filter_shapes)\n",
    "image_shape=(batch_size, 1, img_h, img_w)\n",
    "print('one batch train', image_shape)\n",
    "print(pool_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding & Stride\n",
    "- Output size\n",
    "\n",
    "    $ O = \\frac {W-K+2P}{S} + 1 $\n",
    "    - O: output h/w\n",
    "    - W: input h/w\n",
    "    - K: filter size(kernel size)\n",
    "    - P: padding\n",
    "        - $  P = \\frac {K-1}{2} $\n",
    "    - S: stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "- Theano:\n",
    "    - conv_layer: LeNetConvPoolLayer\n",
    "    - classifier: MLPDropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:\n",
    "\n",
    "\n",
    "```\n",
    "Network\n",
    "Input ->\n",
    "Conv -> ReLU -> MaxPool |\n",
    "Conv -> ReLU -> MaxPool | -> concat\n",
    "Conv -> ReLU -> MaxPool |\n",
    "Fully Connected Layer(Logits -> Softmax) -> Labels\n",
    "```\n",
    "\n",
    "```\n",
    "Convolutional layer formula:\n",
    "- Filter 1(Kernel) size K = 3 => (3 x 300)\n",
    "- P(same padding) P = (3-1)/2=1\n",
    "- S(stride) S = 1\n",
    "- in_channels = 1\n",
    "- out_channels (int) – Number of channels produced by the convolution = 100\n",
    "Pooling layer formula:\n",
    "- K\n",
    "```\n",
    "\n",
    "```\n",
    "*Filter dimensions*:\n",
    "Conv1 (W_conv, (100, 1, 3, 300))\n",
    "Conv1 (b_conv, (100,))\n",
    "Conv2 (W_conv, (100, 1, 4, 300))\n",
    "Conv2 (b_conv, (100,))\n",
    "Conv3 (W_conv, (100, 1, 5, 300))\n",
    "Conv3 (b_conv, (100,))\n",
    "\n",
    "*Layer input dimensions*:\n",
    "- Input image(64, 300) \n",
    "\n",
    "----------------------------------------------------------------------\n",
    "|  Conv1  (100, 3, 300)   Conv2  (100, 4, 300)   Conv3  (100, 5, 300) |\n",
    "|  MaxPool (100, 62, 1)   MaxPool (100, 61, 1)   MaxPool (100, 60, 1) |\n",
    "-----------------------------Concat ----------------------------------\n",
    "\n",
    "- Concatenated (100, 1, 1) + (100, 1, 1) + (100, 1, 1) => (300, 1, 1) \n",
    "\n",
    "- Fully Connected Layer(Logits (100, 1) -> Logits (2, 1) -> Softmax) -> Labels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPoolLayer(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvPoolLayer, self).__init__()\n",
    "\n",
    "        # Layer 1: conv - relu - conv- relu - pool\n",
    "        self.ngram1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(3, 300), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(62, 1), stride=None))\n",
    "        self.ngram2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(4, 300), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(61, 1), stride=None))\n",
    "        self.ngram3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(5, 300), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(60, 1), stride=None))\n",
    "        \n",
    "        # Fully Connected 1 (readout)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(300, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, num_classes))\n",
    "\n",
    "        # Initialize all parameters using kaiming normalization\n",
    "        self.init_weights_kaiming()\n",
    "    \n",
    "    def init_weights_kaiming(self):\n",
    "        #Use kaiming normalization to initialize the parameters\n",
    "        for layer in [self.ngram1, self.ngram2, self.ngram3, self.fc]:\n",
    "            for m in layer:\n",
    "                if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "                    m.weight = nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out1 = self.ngram1(x)\n",
    "        out2 = self.ngram2(x)\n",
    "        out3 = self.ngram3(x)\n",
    "        out = torch.cat((out1, out2, out3), 1)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "    \n",
    "        # Linear function (readout)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvPoolLayer(\n",
      "  (ngram1): Sequential(\n",
      "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(62, 1), stride=(62, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (ngram2): Sequential(\n",
      "    (0): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(61, 1), stride=(61, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (ngram3): Sequential(\n",
      "    (0): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(60, 1), stride=(60, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvPoolLayer(2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 3, 300])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1, 4, 300])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 1, 5, 300])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 300])\n",
      "torch.Size([100])\n",
      "torch.Size([2, 100])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100. Loss: 0.3999162018299103.\n",
      "Iteration: 200. Loss: 0.272891104221344.\n",
      "Iteration: 300. Loss: 0.36832213401794434.\n",
      "Iteration: 400. Loss: 0.10137557238340378.\n",
      "Iteration: 500. Loss: 0.1714673638343811.\n",
      "Iteration: 600. Loss: 0.0668254867196083.\n",
      "Iteration: 700. Loss: 0.04481092840433121.\n",
      "Iteration: 800. Loss: 0.013441810384392738.\n",
      "Iteration: 900. Loss: 0.0074528916738927364.\n",
      "Iteration: 1000. Loss: 0.0026081043761223555.\n",
      "Iteration: 1100. Loss: 0.002037773607298732.\n",
      "Iteration: 1200. Loss: 0.0011451812461018562.\n",
      "Iteration: 1300. Loss: 0.0011072849156334996.\n",
      "Iteration: 1400. Loss: 0.001202746294438839.\n",
      "Iteration: 1500. Loss: 0.0009999914327636361.\n",
      "Iteration: 1600. Loss: 0.0005617720889858902.\n",
      "Iteration: 1700. Loss: 0.0006315104546956718.\n",
      "Iteration: 1800. Loss: 0.0006343545974232256.\n",
      "Iteration: 1900. Loss: 0.0004941895604133606.\n",
      "Iteration: 2000. Loss: 0.0003061308525502682.\n",
      "Iteration: 2100. Loss: 0.00043032041867263615.\n",
      "Iteration: 2200. Loss: 0.0003054253465961665.\n",
      "Iteration: 2300. Loss: 0.00041261970181949437.\n",
      "Iteration: 2400. Loss: 0.0002732417779043317.\n",
      "Iteration: 2500. Loss: 0.0002599158033262938.\n",
      "Iteration: 2600. Loss: 0.0001901353825815022.\n",
      "Iteration: 2700. Loss: 0.00016035525186453015.\n",
      "Iteration: 2800. Loss: 0.00022588316642213613.\n",
      "Iteration: 2900. Loss: 0.0001473306183470413.\n",
      "Iteration: 3000. Loss: 0.0001553473703097552.\n",
      "Iteration: 3100. Loss: 0.00012213025183882564.\n",
      "Iteration: 3200. Loss: 0.00010548481805017218.\n",
      "Iteration: 3300. Loss: 0.0001107609859900549.\n",
      "Iteration: 3400. Loss: 9.87093080766499e-05.\n",
      "Iteration: 3500. Loss: 0.00011185355833731592.\n",
      "Iteration: 3600. Loss: 9.287901048082858e-05.\n",
      "Iteration: 3700. Loss: 8.09331686468795e-05.\n",
      "Iteration: 3800. Loss: 5.5493597756139934e-05.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "avg_losses = list()\n",
    "iter = 0\n",
    "# accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        total_loss = list()\n",
    "        \n",
    "        # Load images as Variable\n",
    "        images = Variable(images) # Now we dont need to resize like images.view(xx)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: Softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t paramters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Track loss to plot the result\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 100 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}.'.format(iter, loss.item()))\n",
    "            avg_loss = np.divide(np.sum(total_loss), len(total_loss))\n",
    "            avg_losses.append(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss vs iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = np.arange(len(avg_losses))\n",
    "plt.plot(x_axis, avg_losses, label='train')\n",
    "plt.xlabel('iterations (100)')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1100 test images: 80.27522935779817 %\n"
     ]
    }
   ],
   "source": [
    "n_test = len(test_loader) * batch_size\n",
    "wrong_predictions = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # See which are error predictions\n",
    "        result = (predicted == labels)\n",
    "        err_imgs = images[result == 0] # 0 means wrong prediction\n",
    "        err_labels = labels[result == 0]\n",
    "        err_outputs = predicted[result == 0]\n",
    "        for img, lbl, out in zip(err_imgs, err_labels, err_outputs):\n",
    "            wrong_predictions.append((img, lbl, out))\n",
    "     \n",
    "    print('Test Accuracy of the model on the {} test images: {} %'.format(n_test, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test memo:\n",
    "\n",
    "#### Model 1\n",
    "|mode/vec|dataset size(train/test)|#epoch|      model  |  optimizer | parameters |lr| accuracy  |\n",
    "|-----|-----------------------|-------|--------------|------------|------------|--|------------|\n",
    "|-nonstatic -rand|9572/1100|20|(conv- relu - pool), (linear)x2 |torch.optim.Adam|-|0.01| 77.15%|\n",
    "|-nonstatic -rand|9572/1100|20|(conv- relu - pool), (linear-relu)x2 |torch.optim.Adam|Kaiming He|0.01| 77.52%|\n",
    "|-nonstatic -rand|9572/1100|20|(conv- relu - pool), (linear-relu)x2 |torch.optim.Adam|Kaiming He|0.001| 80.27%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
