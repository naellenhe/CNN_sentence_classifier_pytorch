{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, re\n",
    "import pandas as pd\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary dictionary : word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_cv(data_folder, cv=10):\n",
    "    \"\"\"\n",
    "    Loads data and split into 10 folds.\n",
    "    \"\"\"\n",
    "    revs = []\n",
    "    label_0_file, label_1_file, label_2_file = data_folder #[\"__label__0.txt\",\"__label__1.txt\", \"__label__2.txt\"]    \n",
    "    word_idx_map = dict()\n",
    "    word_idx = 1\n",
    "    for label, file in enumerate(data_folder): \n",
    "        with codecs.open(file, \"rb\", \"utf-8\", \"ignore\") as f:\n",
    "            for line in f:       \n",
    "                rev = []\n",
    "                rev.append(line.strip())\n",
    "                orig_rev = \" \".join(rev).lower()\n",
    "\n",
    "                words = set(orig_rev.split())\n",
    "                for word in words:\n",
    "                    if word not in word_idx_map:\n",
    "                        word_idx_map[word] = word_idx\n",
    "                        word_idx += 1\n",
    "\n",
    "                datum  = {\"y\": label, \n",
    "                          \"text\": orig_rev,                             \n",
    "                          \"num_words\": len(orig_rev.split()),\n",
    "                          \"split\": np.random.randint(0, cv)}\n",
    "                revs.append(datum)\n",
    "    return revs, word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "data_folder = [\"__label__0.txt\",\"__label__1.txt\", \"__label__2.txt\"]    \n",
    "\n",
    "print(\"loading data...\")      \n",
    "revs, word_idx_map = build_data_cv(data_folder, cv=10)\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset content\n",
    "Get data from twitter (reference: http://tech.wonderpla.net/entry/2017/10/10/110000)\n",
    "- Label 0\n",
    "    - KEYWORD = \"芸能 OR アニメ OR 漫画 OR ドラマ OR ゲーム\"            #エンタメ系のキーワード\n",
    "    - CLASS_LABEL = \"\\__label__0\"\n",
    "\n",
    "- Label 1\n",
    "    - KEYWORD = \"美容 OR サロン OR エステ OR 化粧 OR 保湿\"            #美容系のキーワード\n",
    "    - CLASS_LABEL = \"\\__label__1\"\n",
    "\n",
    "- Label 2\n",
    "    - KEYWORD = \"日常 OR 料理 OR 家事 OR 収納 OR 家具\"            #暮らし系のキーワード\n",
    "    - CLASS_LABEL = \"\\__label__2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['エンタメ系', '美容系', '暮らし系']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>anthem ていう ゲーム warframe と タイタンフォール を 足し て 2 で ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>ナレーター を 無視 する ゲーム です 。 ほんま ツボ # icey</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_words  split                                               text  y\n",
       "682         24      6  anthem ていう ゲーム warframe と タイタンフォール を 足し て 2 で ...  0\n",
       "387         11      5               ナレーター を 無視 する ゲーム です 。 ほんま ツボ # icey  0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(revs)\n",
    "\n",
    "# label 0: Entertainment \n",
    "df[df['y'] == 0].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>月 に 一 度 の 美容 院 は 、 前髪 の カット と 、 カラー と 、 エクステ 頼...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>ダイエット 効果 が ある として 流行 の ヨガ や 太極拳 。 これら の 基本 は 腹...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_words  split                                               text  y\n",
       "790          55      7  月 に 一 度 の 美容 院 は 、 前髪 の カット と 、 カラー と 、 エクステ 頼...  1\n",
       "1017         68      8  ダイエット 効果 が ある として 流行 の ヨガ や 太極拳 。 これら の 基本 は 腹...  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label 1 : Beauty\n",
    "df[df['y'] == 1].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>和牛 水田 「 2 人 で 飯 いっ たら 連れ の 料理 が 先 に 出 た 、 17 分...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>虐待 さ れ て 死ん だ 子供 に は 同情 できる のに 日常 的 に 虐待 さ れ 続...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_words  split                                               text  y\n",
       "1520         31      0  和牛 水田 「 2 人 で 飯 いっ たら 連れ の 料理 が 先 に 出 た 、 17 分...  2\n",
       "1619         78      9  虐待 さ れ て 死ん だ 子供 に は 同情 できる のに 日常 的 に 虐待 さ れ 続...  2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label 2: Life\n",
    "df[df['y'] == 2].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revs 2068\n",
      "word_idx_map 10329\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_idx_map)\n",
    "print('revs',len(revs)) # number of sentence\n",
    "print('word_idx_map', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 0,\n",
       " 'text': 'え 、 サラリーマン し ながら 、 商業 漫画 の 仕事 を し て 、 ツイッター に 定期 的 に 漫画 を あげ て 、 さらに コミケ の 原稿 を 作る ！ ？',\n",
       " 'num_words': 32,\n",
       " 'split': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make BOW vectors\n",
    "make each sentence an BOW vector using word_idx_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_from_sent(revs, word_idx_map, cv, vocab_size):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    train, test = [], []\n",
    "    for rev in revs:\n",
    "        sent = rev[\"text\"]\n",
    "        bow_vec = np.zeros(vocab_size + 1)\n",
    "        bow_vec[0] = rev['y'] # Add true label at the beginning\n",
    "        words = sent.split()\n",
    "        for word in words:\n",
    "            bow_vec[word_idx_map[word]] += 1\n",
    "\n",
    "        if rev[\"split\"]== cv:  # \"split\" is random number of np.random.randint(0,10)\n",
    "            test.append(bow_vec)        \n",
    "        else:  \n",
    "            train.append(bow_vec)   \n",
    "    train = np.array(train, dtype=\"int\")\n",
    "    test = np.array(test, dtype=\"int\")\n",
    "    return [train, test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 0\n",
    "datasets = make_bow_from_sent(revs, word_idx_map, cv, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of sentence to BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 0,\n",
       " 'text': 'かっこいい 、 わくわく する 漫画 も 気 に なっ てる けど なかなか 読め て ない ん だ よ ねぇ 、 満喫 行こ う か な と 思っ て も 家 で ゴロゴロ っと し ちゃう',\n",
       " 'num_words': 35,\n",
       " 'split': 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = datasets[0][:, 1:]\n",
    "train_label = datasets[0][:, 0]\n",
    "test_image = datasets[1][:, 1:]\n",
    "test_label = datasets[1][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: (1874, 10329)\n",
      "train data size: (1874,)\n",
      "test data size: (194, 10329)\n",
      "train data size: (194,)\n"
     ]
    }
   ],
   "source": [
    "print('train data size:', train_image.shape)\n",
    "print('train data size:', train_label.shape)\n",
    "print('test data size:', test_image.shape)\n",
    "print('train data size:', test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_image, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.994845360825\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', clf.score(test_image, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996798292423\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', clf.score(train_image, train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict single inputted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input the text to predict (change the text below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_to_predict = \"お風呂掃除でいつも落ちなかった溝にある黒カビが家事えもんの塩素系漂白剤＋片栗粉でほぼ真っ白になって感動 家事えもんのテクニック凄い～！\"\n",
    "\n",
    "text_to_predict = \"コスメの最安値が見つけられるアプリ💄💋メイク動画とか 美容情報も載ってるし最高😆🙌📲http://goo.gl/K5Fmea 女子にはほんとに助かる〜💗\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_data_from_twitter import process_single_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_from_single_sent(sent, word_idx_map, vocab_size):\n",
    "    \"\"\"\n",
    "    Transforms sentences into a 2-d matrix.\n",
    "    \"\"\"\n",
    "    test = []\n",
    "    bow_vec = np.zeros(vocab_size + 1)\n",
    "    bow_vec[0] = 0 # Add dummy true label at the beginning\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        if word in word_idx_map:\n",
    "            bow_vec[word_idx_map[word]] += 1\n",
    "\n",
    "    test.append(bow_vec)\n",
    "    test = np.array(test, dtype=\"int\")\n",
    "    return test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1行処理済み\n"
     ]
    }
   ],
   "source": [
    "processed_sent = process_single_txt(text_to_predict)\n",
    "single_bow = make_bow_from_single_sent(processed_sent, word_idx_map, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1 美容系\n"
     ]
    }
   ],
   "source": [
    "predected = clf.predict(single_bow[:, 1:]).item()\n",
    "print('Predicted:', predected, label_names[predected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
